{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "séparer par type d'achat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "les clients achètent de manière générale du pgc et des PRODUITS FRAIS :\n",
    "favoriser ce type d'achat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "supprimer textile et bazar des calculs car il n'y a pas trop de recurrence c'est souvent des produits acheter aléatoirement au moment du besoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import dask\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory \n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:20<00:00, 20.58s/it]\n",
      "C:\\Users\\jeann\\AppData\\Local\\Temp\\ipykernel_13160\\3163275204.py:15: DtypeWarning: Columns (43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  products_data = pd.read_csv('data-train/products_data.csv')\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "train_dataframes = []\n",
    "for i in tqdm(range(1, 2)):\n",
    "    train_dataframes.append(pd.read_csv(f'data-train/train_data_part_{i}.csv'))\n",
    "train_data = pd.concat(train_dataframes, ignore_index=True)\n",
    "\n",
    "# free up memory by deleting the dataframes we no longer need\n",
    "del train_dataframes\n",
    "\n",
    "# This code reads the data from a CSV file named \"products_data.csv\" into a pandas DataFrame\n",
    "products_data = pd.read_csv('data-train/products_data.csv')\n",
    "\n",
    "# This code reads the data from a CSV file named \"test_data.csv\" into a pandas DataFrame\n",
    "test_data = pd.read_csv('data-train/test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['date', 'transaction_id', 'customer_id', 'product_id']\n",
    "train_data = train_data[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date       transaction_id   customer_id     product_id  month  \\\n",
      "0 2023-11-15  Transaction_1730125  Household_39   Product_5362     11   \n",
      "1 2022-07-20  Transaction_1560535  Household_39  Product_67174      7   \n",
      "2 2022-07-20  Transaction_1560535  Household_39  Product_82254      7   \n",
      "3 2023-11-15  Transaction_1730125  Household_39   Product_3895     11   \n",
      "4 2022-07-20  Transaction_1560535  Household_39  Product_34014      7   \n",
      "\n",
      "   day_of_week  hour  \n",
      "0            2     0  \n",
      "1            2     0  \n",
      "2            2     0  \n",
      "3            2     0  \n",
      "4            2     0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assurer que la colonne 'date' est en format datetime\n",
    "train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "\n",
    "# Extraire les informations temporelles\n",
    "train_data['month'] = train_data['date'].dt.month        # Mois de l'année\n",
    "train_data['day_of_week'] = train_data['date'].dt.weekday    # jour de la semaine (0 = lundi, 6 = dimanche)\n",
    "l'information sur les jours de la semaine aurai pu etre interéssante mais pour les données test nous ne savons pas sur quel jour ce passe les achats \n",
    "\n",
    "# Afficher un aperçu des données\n",
    "print(train_data.head())\n",
    "\n",
    "# Suppression de la colonne date si elle n'est plus nécessaire pour éviter la multicolinéarité\n",
    "#train_data = train_data.drop(columns=['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              date       transaction_id  customer_id     product_id  month  \\\n",
      "1256210 2022-03-01  Transaction_2515147  Household_1  Product_62373      3   \n",
      "1521128 2022-03-01  Transaction_2515147  Household_1  Product_33145      3   \n",
      "2048302 2022-03-01  Transaction_2515147  Household_1  Product_68212      3   \n",
      "3378083 2022-03-01  Transaction_2515147  Household_1  Product_62388      3   \n",
      "3378089 2022-03-01  Transaction_2515147  Household_1  Product_62136      3   \n",
      "\n",
      "         day_of_week  hour  \n",
      "1256210            1     0  \n",
      "1521128            1     0  \n",
      "2048302            1     0  \n",
      "3378083            1     0  \n",
      "3378089            1     0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeann\\AppData\\Local\\Temp\\ipykernel_13160\\835597002.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  first_10_purchases['date'] = pd.to_datetime(first_10_purchases['date'])\n",
      "C:\\Users\\jeann\\AppData\\Local\\Temp\\ipykernel_13160\\835597002.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  first_10_purchases['month'] = first_10_purchases['date'].dt.month\n"
     ]
    }
   ],
   "source": [
    "# Trier les données par customer_id et date\n",
    "train_data = train_data.sort_values(by=['customer_id', 'date'])\n",
    "\n",
    "# Extraire les 10 premières transactions de chaque client\n",
    "first_50_purchases = train_data.groupby('customer_id').head(50)\n",
    "\n",
    "# Extraire des variables temporelles des dates\n",
    "first_50_purchases['date'] = pd.to_datetime(first_50_purchases['date'])\n",
    "first_50_purchases['month'] = first_50_purchases['date'].dt.month\n",
    "\n",
    "# Vérifier les données\n",
    "print(first_50_purchases.head())\n",
    "\n",
    "# Optionnel : Suppression de la colonne date si non nécessaire\n",
    "#first_10_purchases = first_10_purchases.drop(columns=['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "# Séparer X et y\n",
    "X = train_data.drop(columns=[\"product_id\"])  # Toutes les colonnes sauf product_id\n",
    "y = train_data[\"product_id\"]  # La cible (product_id)\n",
    "\n",
    "# Encodage de la variable cible (product_id)\n",
    "product_encoder = LabelEncoder()\n",
    "y_encoded = product_encoder.fit_transform(y)\n",
    "\n",
    "# Encodage des colonnes catégorielles dans X\n",
    "categorical_columns = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "# Appliquer OneHotEncoder sur les colonnes catégorielles\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\") # sparse was deprecated in 1.2 and replaced with sparse_output\n",
    "X_encoded = pd.DataFrame(encoder.fit_transform(X[categorical_columns]))\n",
    "\n",
    "# Ajouter les colonnes numériques\n",
    "numerical_columns = X.select_dtypes(exclude=[\"object\"]).columns\n",
    "X_encoded = X_encoded.astype(np.float32)\n",
    "\n",
    "# Conversion en matrices numpy\n",
    "X_train = X_encoded.to_numpy()\n",
    "y_train = to_categorical(y_encoded, num_classes=len(product_encoder.classes_))\n",
    "\n",
    "print(f\"Dimensions de X_train : {X_train.shape}\")\n",
    "print(f\"Dimensions de y_train : {y_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodage des données de test\n",
    "X_test = test_data[['transaction_id', 'customer_id']]  # Select only the columns present in test_data and used for training\n",
    "\n",
    "# Get categorical columns present in both train and test data\n",
    "categorical_columns_test = list(set(categorical_columns) & set(X_test.columns))\n",
    "\n",
    "# Create missing columns in X_test and fill with a placeholder value (e.g., 0)\n",
    "# We fill with 0 because the numerical columns are related to the training data and do not exist in the test data.\n",
    "# Instead of 'missing', we use 0 to represent the absence of these features in the test data.\n",
    "for col in numerical_columns:\n",
    "    if col not in X_test.columns:\n",
    "        X_test[col] = 0\n",
    "\n",
    "# Create missing columns in X_test and fill with a placeholder value (e.g., 'missing')\n",
    "for col in categorical_columns:\n",
    "    if col not in X_test.columns:\n",
    "        X_test[col] = 'missing'  # Or any other suitable placeholder\n",
    "\n",
    "# Now you can safely apply the transform\n",
    "X_test_encoded = pd.DataFrame(encoder.transform(X_test[categorical_columns]))\n",
    "\n",
    "# Continue with the rest of your code...\n",
    "x_test_final = pd.concat([X_test_encoded, X_test[numerical_columns].reset_index(drop=True)], axis=1)\n",
    "X_test_final = x_test_final.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "num_features = X_train.shape[1]  # Nombre de colonnes dans X\n",
    "num_products = y_train.shape[1]  # Nombre de classes (produits)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=num_features, activation=\"relu\"))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(num_products, activation=\"softmax\"))  # Prédictions des produits\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Entraîner le modèle\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_product(model, top_n=10):\n",
    "    \"\"\"\n",
    "    Génère les produits recommandés pour chaque instance dans X_train.\n",
    "\n",
    "    Args:\n",
    "        model: Le modèle entraîné.\n",
    "        top_n: Le nombre de produits recommandés (par défaut : 10).\n",
    "\n",
    "    Returns:\n",
    "        Une liste de produits recommandés pour chaque exemple d'entraînement.\n",
    "    \"\"\"\n",
    "    # Prédire les probabilités pour les données d'entraînement\n",
    "    prediction = model.predict(X_train)\n",
    "\n",
    "    # Obtenir les indices des produits les plus probables\n",
    "    top_products = np.argsort(prediction, axis=1)[:, -top_n:][:, ::-1]\n",
    "\n",
    "    # Décoder les produits pour retourner les identifiants originaux\n",
    "    return [product_encoder.inverse_transform(top_products[i]) for i in range(len(top_products))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Générer les recommandations\n",
    "predicted_products = predict_product(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hitrate_at_k(test_data, predicted_products, k=10):\n",
    "    \"\"\"\n",
    "    Calcule le Hitrate@K.\n",
    "\n",
    "    Args:\n",
    "        test_data: Le jeu de données de test contenant les produits réellement achetés.\n",
    "        predicted_products: Liste des produits recommandés par le modèle.\n",
    "        k: Le nombre de recommandations à considérer (par défaut : 10).\n",
    "\n",
    "    Returns:\n",
    "        Le score Hitrate@K.\n",
    "    \"\"\"\n",
    "    hits = 0\n",
    "\n",
    "    for i, true_product in enumerate(test_data[\"product_id\"]):\n",
    "        if true_product in predicted_products[i][:k]:  # Vérifier si le vrai produit est dans les top K recommandations\n",
    "            hits += 1\n",
    "\n",
    "    return hits / len(test_data)\n",
    "\n",
    "\n",
    "# Calcul du Hitrate@10\n",
    "hitrate = hitrate_at_k(test_data, predicted_products, k=10)\n",
    "print(f\"Hitrate@10 : {hitrate:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
